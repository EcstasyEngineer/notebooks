{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch_vggish_yamnet import yamnet\n",
    "from torch_vggish_yamnet.input_proc import WaveformToInput\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "import subprocess\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YAMNet(\n",
       "  (layer1): Conv(\n",
       "    (fused): CONV_BN_RELU(\n",
       "      (conv): Conv2d_tf(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=SAME, bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer2): SeparableConv(\n",
       "    (depthwise_conv): CONV_BN_RELU(\n",
       "      (conv): Conv2d_tf(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=SAME, groups=32, bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (pointwise_conv): CONV_BN_RELU(\n",
       "      (conv): Conv2d_tf(32, 64, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer3): SeparableConv(\n",
       "    (depthwise_conv): CONV_BN_RELU(\n",
       "      (conv): Conv2d_tf(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=SAME, groups=64, bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (pointwise_conv): CONV_BN_RELU(\n",
       "      (conv): Conv2d_tf(64, 128, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer4): SeparableConv(\n",
       "    (depthwise_conv): CONV_BN_RELU(\n",
       "      (conv): Conv2d_tf(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=SAME, groups=128, bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (pointwise_conv): CONV_BN_RELU(\n",
       "      (conv): Conv2d_tf(128, 128, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer5): SeparableConv(\n",
       "    (depthwise_conv): CONV_BN_RELU(\n",
       "      (conv): Conv2d_tf(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=SAME, groups=128, bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (pointwise_conv): CONV_BN_RELU(\n",
       "      (conv): Conv2d_tf(128, 256, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer6): SeparableConv(\n",
       "    (depthwise_conv): CONV_BN_RELU(\n",
       "      (conv): Conv2d_tf(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=SAME, groups=256, bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (pointwise_conv): CONV_BN_RELU(\n",
       "      (conv): Conv2d_tf(256, 256, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer7): SeparableConv(\n",
       "    (depthwise_conv): CONV_BN_RELU(\n",
       "      (conv): Conv2d_tf(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=SAME, groups=256, bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (pointwise_conv): CONV_BN_RELU(\n",
       "      (conv): Conv2d_tf(256, 512, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer8): SeparableConv(\n",
       "    (depthwise_conv): CONV_BN_RELU(\n",
       "      (conv): Conv2d_tf(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=SAME, groups=512, bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (pointwise_conv): CONV_BN_RELU(\n",
       "      (conv): Conv2d_tf(512, 512, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer9): SeparableConv(\n",
       "    (depthwise_conv): CONV_BN_RELU(\n",
       "      (conv): Conv2d_tf(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=SAME, groups=512, bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (pointwise_conv): CONV_BN_RELU(\n",
       "      (conv): Conv2d_tf(512, 512, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer10): SeparableConv(\n",
       "    (depthwise_conv): CONV_BN_RELU(\n",
       "      (conv): Conv2d_tf(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=SAME, groups=512, bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (pointwise_conv): CONV_BN_RELU(\n",
       "      (conv): Conv2d_tf(512, 512, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer11): SeparableConv(\n",
       "    (depthwise_conv): CONV_BN_RELU(\n",
       "      (conv): Conv2d_tf(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=SAME, groups=512, bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (pointwise_conv): CONV_BN_RELU(\n",
       "      (conv): Conv2d_tf(512, 512, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer12): SeparableConv(\n",
       "    (depthwise_conv): CONV_BN_RELU(\n",
       "      (conv): Conv2d_tf(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=SAME, groups=512, bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (pointwise_conv): CONV_BN_RELU(\n",
       "      (conv): Conv2d_tf(512, 512, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer13): SeparableConv(\n",
       "    (depthwise_conv): CONV_BN_RELU(\n",
       "      (conv): Conv2d_tf(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=SAME, groups=512, bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (pointwise_conv): CONV_BN_RELU(\n",
       "      (conv): Conv2d_tf(512, 1024, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
       "      (bn): BatchNorm2d(1024, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer14): SeparableConv(\n",
       "    (depthwise_conv): CONV_BN_RELU(\n",
       "      (conv): Conv2d_tf(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=SAME, groups=1024, bias=False)\n",
       "      (bn): BatchNorm2d(1024, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (pointwise_conv): CONV_BN_RELU(\n",
       "      (conv): Conv2d_tf(1024, 1024, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
       "      (bn): BatchNorm2d(1024, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=1024, out_features=521, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the waveform converter\n",
    "converter = WaveformToInput()\n",
    "\n",
    "# Initialize the YAMNet model\n",
    "model = yamnet.yamnet(pretrained=True)\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_audio(file_path, target_sr=16000):\n",
    "#     # Load audio file\n",
    "#     audio = AudioSegment.from_file(file_path, format=\"webm\")\n",
    "#     print(1 )\n",
    "#     # Resample to target sample rate\n",
    "#     audio = audio.set_frame_rate(target_sr)\n",
    "#     print(2)\n",
    "#     # If stereo, average the channels to create mono\n",
    "#     if audio.channels > 1:\n",
    "#         samples = np.array(audio.split_to_mono())\n",
    "#         averaged_samples = (samples[0].get_array_of_samples() + samples[1].get_array_of_samples()) / 2\n",
    "#     else:\n",
    "#         averaged_samples = np.array(audio.get_array_of_samples())\n",
    "#     print(3)\n",
    "#     # Convert to float32 and normalize\n",
    "#     y = averaged_samples.astype(np.float32)\n",
    "#     y /= np.iinfo(audio.array_type).max  # Normalize to [-1, 1]\n",
    "#     print(4)\n",
    "#     return y, target_sr\n",
    "\n",
    "def load_audio(file_path, target_sr=16000):\n",
    "    import torchaudio\n",
    "\n",
    "    # Load the audio file\n",
    "    y, sr = torchaudio.load(file_path)  # y is a tensor of shape [channels, samples]\n",
    "\n",
    "    print(f\"Loaded audio file {file_path}, sample rate {sr}, waveform shape {y.shape}\")\n",
    "\n",
    "    # Convert to mono by averaging channels if necessary\n",
    "    if y.shape[0] > 1:\n",
    "        y = y.mean(dim=0)\n",
    "    else:\n",
    "        y = y.squeeze(0)\n",
    "\n",
    "    # Resample if needed\n",
    "    if sr != target_sr:\n",
    "        resampler = torchaudio.transforms.Resample(sr, target_sr)\n",
    "        y = resampler(y)\n",
    "        sr = target_sr\n",
    "\n",
    "    y = y.numpy()  # Convert tensor to numpy array if needed\n",
    "    return y, sr\n",
    "\n",
    "def convert_webm_to_wav(file_path):\n",
    "    # Define the output file path with a .wav extension\n",
    "    output_path = file_path.replace(\".webm\", \".wav\")\n",
    "    # Run the ffmpeg command to convert\n",
    "    subprocess.run(['ffmpeg', '-i', file_path, output_path, '-y'], check=True)\n",
    "    return output_path\n",
    "\n",
    "def detect_laughter(y_chunk, sr, threshold=0.5):\n",
    "    y = torch.tensor(y_chunk).float().to(device)  # Move to the correct device\n",
    "\n",
    "    # Ensure y has the correct shape [batch_size, samples]\n",
    "    if y.ndim == 1:\n",
    "        y = y.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    in_tensor = converter(y, sr).to(device)  # Move the input tensor to the same device as the model\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings, logits = model(in_tensor)\n",
    "\n",
    "    probabilities = torch.softmax(logits, dim=1).cpu()  # Move to CPU for processing\n",
    "\n",
    "    class_map_path = yamnet._DEFAULT_YAMNET_CLASSES_PATH\n",
    "    with open(class_map_path, 'r') as f:\n",
    "        class_names = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    laughter_indices = [i for i, name in enumerate(class_names) if 'laughter' in name.lower()]\n",
    "    if not laughter_indices:\n",
    "        raise ValueError(\"Laughter class not found in YAMNet class names.\")\n",
    "    laughter_index = laughter_indices[0]\n",
    "\n",
    "    frame_duration = 0.48\n",
    "    timestamps = np.arange(len(probabilities)) * frame_duration\n",
    "\n",
    "    laughter_probs = probabilities[:, laughter_index]\n",
    "    laughter_frames = laughter_probs > threshold\n",
    "\n",
    "    laughter_events = []\n",
    "    start_time = None\n",
    "    for i, is_laughter in enumerate(laughter_frames):\n",
    "        time = timestamps[i]\n",
    "        if is_laughter and start_time is None:\n",
    "            start_time = time\n",
    "        elif not is_laughter and start_time is not None:\n",
    "            end_time = time\n",
    "            laughter_events.append((start_time, end_time))\n",
    "            start_time = None\n",
    "    if start_time is not None:\n",
    "        laughter_events.append((start_time, timestamps[-1]))\n",
    "\n",
    "    return laughter_events\n",
    "\n",
    "def save_laughter_events(laughter_events, output_file):\n",
    "    df = pd.DataFrame(laughter_events, columns=['Start Time', 'End Time'])\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "def process_audio_file(file_path, output_file, threshold=0.5):\n",
    "    y, sr = load_audio(file_path)\n",
    "    laughter_events = detect_laughter(y, sr, threshold=threshold)\n",
    "    save_laughter_events(laughter_events, output_file)\n",
    "    print(f\"Laughter events saved to {output_file}\")\n",
    "    \n",
    "def process_audio_in_chunks(file_path, chunk_duration=10, threshold=0.5):\n",
    "    print(\"Loading audio file...\")\n",
    "    y, sr = load_audio(file_path)\n",
    "    total_duration = len(y) / sr\n",
    "    laughter_events = []\n",
    "    total_chunks = int(np.ceil(total_duration / chunk_duration))\n",
    "    print(f\"Processing audio in {chunk_duration}-second chunks...\")\n",
    "\n",
    "    for i, start in enumerate(np.arange(0, total_duration, chunk_duration)):\n",
    "        print(f\"Processing chunk {i+1} of {total_chunks}...\")\n",
    "        end = min(start + chunk_duration, total_duration)\n",
    "        start_sample = int(start * sr)\n",
    "        end_sample = int(end * sr)\n",
    "        y_chunk = y[start_sample:end_sample]\n",
    "\n",
    "        # Process the chunk and collect laughter events\n",
    "        events = detect_laughter(y_chunk, sr, threshold)\n",
    "\n",
    "        # Adjust event times for the full audio timeline\n",
    "        adjusted_events = [(s + start, e + start) for s, e in events]\n",
    "        laughter_events.extend(adjusted_events)\n",
    "\n",
    "    # Save the aggregated laughter events\n",
    "    output_file = f\"{file_path}_laughter_timestamps.csv\"\n",
    "    save_laughter_events(laughter_events, output_file)\n",
    "    print(f\"Laughter events saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading audio file...\n",
      "Loaded audio file @StronnyCuttles/processed/ã€ASMR Streamã€‘Intense Ear Cleaning and Slime Poking!ðŸ¦‘ðŸ›ã€VAllureã€‘ [3rIax65ailI].wav, sample rate 48000, waveform shape torch.Size([2, 357600305])\n",
      "Processing audio in 10-second chunks...\n",
      "Processing chunk 1 of 746...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch_vggish_yamnet.yamnet' has no attribute '_DEFAULT_YAMNET_CLASSES_PATH'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_laughter_timestamps.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#process_audio_file(file_path, output_file, threshold=0.5)\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mprocess_audio_in_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_duration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# delete the converted file if it was created\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m delete_file:\n",
      "Cell \u001b[1;32mIn[5], line 121\u001b[0m, in \u001b[0;36mprocess_audio_in_chunks\u001b[1;34m(file_path, chunk_duration, threshold)\u001b[0m\n\u001b[0;32m    118\u001b[0m y_chunk \u001b[38;5;241m=\u001b[39m y[start_sample:end_sample]\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Process the chunk and collect laughter events\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m events \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_laughter\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Adjust event times for the full audio timeline\u001b[39;00m\n\u001b[0;32m    124\u001b[0m adjusted_events \u001b[38;5;241m=\u001b[39m [(s \u001b[38;5;241m+\u001b[39m start, e \u001b[38;5;241m+\u001b[39m start) \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m events]\n",
      "Cell \u001b[1;32mIn[5], line 65\u001b[0m, in \u001b[0;36mdetect_laughter\u001b[1;34m(y_chunk, sr, threshold)\u001b[0m\n\u001b[0;32m     61\u001b[0m     embeddings, logits \u001b[38;5;241m=\u001b[39m model(in_tensor)\n\u001b[0;32m     63\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()  \u001b[38;5;66;03m# Move to CPU for processing\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m class_map_path \u001b[38;5;241m=\u001b[39m \u001b[43myamnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_DEFAULT_YAMNET_CLASSES_PATH\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(class_map_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     67\u001b[0m     class_names \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mreadlines()]\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch_vggish_yamnet.yamnet' has no attribute '_DEFAULT_YAMNET_CLASSES_PATH'"
     ]
    }
   ],
   "source": [
    "audio_files = ['@StronnyCuttles/processed/ã€ASMR Streamã€‘Intense Ear Cleaning and Slime Poking!ðŸ¦‘ðŸ›ã€VAllureã€‘ [3rIax65ailI].webm']  # Replace with your list of audio files\n",
    "# for file_path in audio_files:\n",
    "#     output_file = f\"{file_path}_laughter_timestamps.csv\"\n",
    "#     #process_audio_file(file_path, output_file, threshold=0.5)\n",
    "#     process_audio_in_chunks(file_path, chunk_duration=10, threshold=0.5)\n",
    "\n",
    "\n",
    "for file_path in audio_files:\n",
    "    delete_file = False\n",
    "    if file_path.endswith('.webm'):\n",
    "        file_path = convert_webm_to_wav(file_path)  # Convert if webm format\n",
    "        delete_file = True\n",
    "        \n",
    "    output_file = f\"{file_path}_laughter_timestamps.csv\"\n",
    "    #process_audio_file(file_path, output_file, threshold=0.5)\n",
    "    process_audio_in_chunks(file_path, chunk_duration=10, threshold=0.5)\n",
    "    # delete the converted file if it was created\n",
    "    if delete_file:\n",
    "        print(f\"Deleting converted file {file_path}\")\n",
    "        #os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_excessive_laughter(laughter_events, min_duration=2.0):\n",
    "    return [event for event in laughter_events if (event[1] - event[0]) >= min_duration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@StronnyCuttles/processed/ã€ASMR Streamã€‘Intense Ear Cleaning and Slime Poking!ðŸ¦‘ðŸ›ã€VAllureã€‘ [3rIax65ailI].webm']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m laughter_events \u001b[38;5;241m=\u001b[39m detect_laughter(\u001b[43my\u001b[49m, sr, threshold\u001b[38;5;241m=\u001b[39mthreshold)\n\u001b[0;32m      2\u001b[0m excessive_laughter_events \u001b[38;5;241m=\u001b[39m filter_excessive_laughter(laughter_events, min_duration\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m)\n\u001b[0;32m      3\u001b[0m save_laughter_events(excessive_laughter_events, output_file)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "laughter_events = detect_laughter(y, sr, threshold=threshold)\n",
    "excessive_laughter_events = filter_excessive_laughter(laughter_events, min_duration=2.0)\n",
    "save_laughter_events(excessive_laughter_events, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
